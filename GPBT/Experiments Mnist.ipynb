{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20246,
     "status": "ok",
     "timestamp": 1617488631372,
     "user": {
      "displayName": "Julien Scardigli",
      "photoUrl": "",
      "userId": "16471186942324360260"
     },
     "user_tz": -120
    },
    "id": "e6G7RhE7s34d",
    "outputId": "12f2ef85-2672-43be-850e-109abc009761"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/gpbthebo/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import random       \n",
    "import math \n",
    "from ray import tune\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from functools import *\n",
    "from ray.tune.logger import *\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, \\\n",
    "            select_length, UCB, standardize, TV_SquaredExp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMON CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56756,
     "status": "ok",
     "timestamp": 1617488691939,
     "user": {
      "displayName": "Julien Scardigli",
      "photoUrl": "",
      "userId": "16471186942324360260"
     },
     "user_tz": -120
    },
    "id": "B6SaMRN0rPDg",
    "outputId": "ca452473-cf07-4a43-c592-5e3bb8595076"
   },
   "outputs": [],
   "source": [
    "EPOCH_SIZE = 32*32*32*32\n",
    "TEST_SIZE = 256*32*32 #remove 1024\n",
    "\n",
    "#This is a function that can be used by several NN model\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#This is a function that can be used by several NN model (it only does accuracy ATM)\n",
    "def test(model, func, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()  \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DF_euOzyrPDn"
   },
   "outputs": [],
   "source": [
    "# A random mnist from the internet to get a correct model to reason about\n",
    "\n",
    "class train_mnist():\n",
    "    def __init__(self,config):\n",
    "        self.DEFAULT_PATH = \".temp/data/\"\n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":43\n",
    "      ,  \"n_layer\":2    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "       # mnist_transforms = transforms.Compose(\n",
    "       #     [transforms.ToTensor(),\n",
    "       #      transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.MNIST(self.DEFAULT_PATH, train=True, download=True , transform=mnist_transforms),\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.model = LeNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def adapt(self, config):\n",
    "        #print(self.optimizer)\n",
    "        temp = copy.deepcopy(self)\n",
    "        for key, value in config.items():\n",
    "            temp.config[key] = value\n",
    "        config = temp.config\n",
    "\n",
    "        temp.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "        temp.optimizer = torch.optim.Adam(temp.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        self.train1()\n",
    "        return self.val1()\n",
    "\n",
    "# __INCEPTION_SCORE_begin__\n",
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "            drop_prob, sigmoid ):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def adapt(self,drop_prob):\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function, evals): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.rand.suggest), max_evals=evals, trials=Trials())\n",
    "\n",
    "class RandomLogger(tune.logger.Logger):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        timestamp = datetime.time().strftime(\"%H_%M_%d_%m_%Y\")\n",
    "        directory = os.path.join(\".temp/data\", \"MNIST\", timestamp)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = \"randomopt\" + \"_\" + \"LeNet\" + \"_\" + str(0) + \".csv\"\n",
    "        progress_file = os.path.join(directory, filename)\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal Random Opt\n",
    "def function(x, hyperlogger):\n",
    "    model = train_mnist(x)\n",
    "    for _ in range(2): # Iterations\n",
    "        model.train1()\n",
    "        loss = model.test1()\n",
    "        test = model.val1()\n",
    "        temp = dict(x)\n",
    "        temp.update({'loss' : loss})\n",
    "        temp.update({'test' : test})\n",
    "\n",
    "        temp.update({'iteration' :  model.i})\n",
    "        hyperlogger.on_result(result=temp)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: .temp/data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting .temp/data/MNIST/raw/train-images-idx3-ubyte.gz to .temp/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: .temp/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting .temp/data/MNIST/raw/train-labels-idx1-ubyte.gz to .temp/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: .temp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting .temp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to .temp/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: .temp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting .temp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to .temp/data/MNIST/raw\n",
      "iteration: 0                                         \n",
      "iteration: 1                                         \n",
      "100%|██████████| 1/1 [00:18<00:00, 18.68s/trial, best loss: -0.9474]\n",
      "totalt time: 18.689531087875366\n"
     ]
    }
   ],
   "source": [
    "evals = 1  # num_configurations\n",
    "\n",
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1),\n",
    "     \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1),\n",
    "     \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "}\n",
    "\n",
    "oracle = RandomOpt(config)\n",
    "hyperlogger = RandomLogger(config)\n",
    "start_time = time.time()\n",
    "\n",
    "fmin_objective = partial(function, hyperlogger=hyperlogger)\n",
    "oracle.compute_Once(fmin_objective, evals)\n",
    "print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function, evals): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=evals, trials=Trials())\n",
    "    \n",
    "\n",
    "class HyperLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\"./tmp/data\", \"hyperopt_v2.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b1': 0.9024406110046069, 'b2': 0.9980596857161873, 'droupout_prob': 0.5119873615752234, 'lr': 0.057282390741965485, 'weight_decay': 0.017274050306771573}\n",
      "iteration: 0                                         \n",
      "iteration: 1                                         \n",
      "100%|██████████| 1/1 [00:18<00:00, 18.56s/trial, best loss: -0.113]\n",
      "totalt time: 18.567007064819336\n"
     ]
    }
   ],
   "source": [
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1),\n",
    "     \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1),\n",
    "     \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "    \n",
    "}\n",
    "\n",
    "hyperlogger = HyperLogger(config,\"\")\n",
    "\n",
    "for _ in range(1):\n",
    "  oracle = BayesOpt(config)\n",
    "  start_time = time.time()\n",
    "  fmin_objective = partial(function, hyperlogger=hyperlogger)\n",
    "  oracle.compute_Once(fmin_objective, evals)\n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import *\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\".temp/data\", \"BOHB_LeNet.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "                  }\n",
    "\n",
    "for _ in range(1):\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "    start_time = time.time()\n",
    "    algo = TuneBOHB(metric=\"loss\", mode=\"max\")\n",
    "    bohb = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        max_t=10)\n",
    "    analysis = tune.run(train_mnist_pb2, config=config, scheduler=bohb, search_alg=algo,num_samples=1, loggers=[TestLogger],resources_per_trial={'cpu':8 ,'gpu': 1})\n",
    "\n",
    "    print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler \n",
    "from ray.tune.schedulers import MedianStoppingRule\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "#import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "class train_mnist_pb2(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.DEFAULT_PATH = \"./tmp/data\"\n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":43\n",
    "      ,  \"n_layer\":2    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "        #mnist_transforms = transforms.Compose(\n",
    "        #    [transforms.ToTensor(),\n",
    "        #     transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.MNIST(self.DEFAULT_PATH, train=True, download=True, transform=mnist_transforms),\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.model = LeNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        if config.get(\"b1\", 0.999)>=1:\n",
    "          temp = 1 - 1e-10\n",
    "        else:\n",
    "          temp = config.get(\"b1\", 0.999)\n",
    "                \n",
    "        if config.get(\"b2\", 0.999)>=1:\n",
    "          temp1 = 1 - 1e-10\n",
    "        else:\n",
    "          temp1 = config.get(\"b2\", 0.999)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((temp,temp1)),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def reset_config(self, config):\n",
    "        if \"lr\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"lr\"] = config.get(\"lr\", 0.01)\n",
    "        if \"b1\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"betas\"] = (\n",
    "                    (1-1e-10 if config.get(\"b1\", 0.999)>=1 else config.get(\"b1\", 0.999),(1-1e-10 if config.get(\"b2\", 0.999)>=1 else config.get(\"b2\", 0.999))))\n",
    "        if \"weight_decay\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"weight_decay\"] = config.get(\"weight_decay\", 0) \n",
    "        \n",
    "        self.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "\n",
    "        self.config = config\n",
    "        return True\n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        self.train1()\n",
    "        return {'loss' : self.test1(), 'test' : self.val1()}\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optim\": self.optimizer.state_dict(),\n",
    "\n",
    "        }, path)\n",
    "\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optim\"])\n",
    "\n",
    "\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\".temp/data\", \"PB2_2_6.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "algo = ConcurrencyLimiter(HyperOptSearch(metric=\"loss\",\n",
    "    mode=\"max\"), max_concurrent=25)\n",
    "scheduler = PB2(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=2,\n",
    "    hyperparam_bounds={\n",
    "        # distribution for resampling\n",
    "     \"lr\": [0+epsilon, .1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[0+epsilon, .1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [.9, 1-epsilon]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [.99, 1-epsilon] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "    }) \n",
    "\n",
    "imageSize = 32\n",
    "cst=imageSize\n",
    "total=cst*cst*3\n",
    "def trial_name_id(trial):\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "  ray.shutdown()\n",
    "  ray.init()\n",
    "  start_time = time.time()\n",
    "  analysis = tune.run(   \n",
    "  train_mnist_pb2,\n",
    "  scheduler=scheduler,\n",
    "  reuse_actors=False,\n",
    "  search_alg=algo,\n",
    "  verbose=2,\n",
    "  checkpoint_at_end=True,\n",
    "  num_samples=28,\n",
    "  # export_formats=[ExportFormat.MODEL],\n",
    "              config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "          \n",
    "      },      stop={\n",
    "          \"training_iteration\": 10,\n",
    "      },        metric=\"loss\",\n",
    "      mode=\"max\"\n",
    ",resources_per_trial={'cpu':8 ,'gpu': 1}\n",
    "              ,     loggers=[TestLogger])\n",
    "  print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.examples.pbt_function import pbt_function\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    class TestLogger(tune.logger.Logger):\n",
    "        def _init(self):\n",
    "            progress_file = os.path.join(path, \"PBT\"+str(i)+\".csv\")\n",
    "            self._continuing = os.path.exists(progress_file)\n",
    "            self._file = open(progress_file, \"a\")\n",
    "            self._csv_out = None\n",
    "        def on_result(self, result):\n",
    "            tmp = result.copy()\n",
    "            result = flatten_dict(tmp, delimiter=\"/\")\n",
    "            if self._csv_out is None:\n",
    "                self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "                self._csv_out.writeheader()\n",
    "            self._csv_out.writerow(\n",
    "                {k: v\n",
    "                 for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "            self._file.flush()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    ray.shutdown()\n",
    "    ray.init()  # force pausing to happen for test\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "    algo = HyperOptSearch(metric=\"loss\",\n",
    "        mode=\"max\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        perturbation_interval=1,\n",
    "            time_attr=\"training_iteration\",\n",
    "\n",
    "        hyperparam_mutations={\n",
    "            # hyperparameter bounds.\n",
    "     \"lr\": [0+epsilon, .1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[0+epsilon, .1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [.9, 1-epsilon]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [.99, 1-epsilon] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "        })\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_mnist_pb2,\n",
    "  checkpoint_at_end=True,\n",
    "        scheduler=pbt,\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        search_alg = algo,\n",
    "        verbose=0,\n",
    "        stop={\n",
    "            \"training_iteration\": 10,\n",
    "        },\n",
    "        num_samples=25,\n",
    "  reuse_actors=True,\n",
    "loggers=[TestLogger],\n",
    "        \n",
    "          config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "                      },resources_per_trial={'cpu':0 ,'gpu': 1})\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(x,models,h,losses, parent_model,k_f,iteration,fsvnlogger):\n",
    "    if (isinstance(k_f,list)):\n",
    "            k=k_f[0]\n",
    "            Islist = True \n",
    "    else:\n",
    "            k = k_f\n",
    "            Islist = False\n",
    "        \n",
    "    if iteration == 0:\n",
    "        models[k] = parent_model[k](x)\n",
    "    else:      \n",
    "        models[k] = parent_model.adapt(x)\n",
    "    if(Islist):\n",
    "        k_f[0] += 1\n",
    "    \n",
    "    #for key, value in x.items():\n",
    "    #        print(key + \" \"+str(x[key]))\n",
    "\n",
    "    h[k] = x\n",
    "    #start_time = time.time()\n",
    "    models[k].train1()\n",
    "    loss = models[k].test1()\n",
    "    test = models[k].val1()\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    temp = dict(x)\n",
    "    temp.update({'loss' : loss})\n",
    "    temp.update({'test' : test})\n",
    "    fsvnlogger.on_result(temp)\n",
    "\n",
    "    losses[k] = -loss\n",
    "    print(\"accuracy, \" + str(loss) + \"\\n\")\n",
    "    return -loss\n",
    "\n",
    "\n",
    "\n",
    "class Parent():\n",
    "    \"\"\"Parent Class that handles the passage of Network Configurations from one step to the\n",
    "    following\n",
    "    \"\"\"\n",
    "    def __init__(self, point_hyperspace, configuration, model, loss):\n",
    "        self.point_hyperspace = point_hyperspace\n",
    "        self.configuration_list = [configuration]\n",
    "        self.loss_list = [np.array(loss)]\n",
    "        self.model = model\n",
    "        self.is_replicated = False\n",
    "\n",
    "    def update(self, configuration, loss, model):\n",
    "        self.is_replicated = False\n",
    "        self.configuration_list.append(configuration)\n",
    "        self.loss_list=np.append(self.loss_list,loss)\n",
    "        self.model = model\n",
    "\n",
    "    def replication(self, n_children):\n",
    "        self.is_replicated = True\n",
    "      #  self.configuration_list.append(self.configuration_list[-1])\n",
    "      #  self.loss_list=np.append(self.loss_list,self.loss_list[-1])\n",
    "    #    replication_trials(self.point_hyperspace.trials, n_children)\n",
    "\n",
    "    def get_last_conf(self):\n",
    "        return self.configuration_list[-1]\n",
    "\n",
    "    def get_point_hyperspace(self):\n",
    "        return self.point_hyperspace\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss_list\n",
    "\n",
    "class Scheduler():\n",
    "    def __init__(self, model, num_iteration, num_config,\n",
    "                 oracle):\n",
    "        #Oracle manages the Bayesian optimization\n",
    "        self.oracle = oracle\n",
    "        self.iteration = num_iteration\n",
    "        self.num_config = num_config \n",
    "        self.sqrt_config =   math.floor(math.sqrt(num_config)) # math.ceil(num_config/5) #\n",
    "        self.n_parents = self.sqrt_config\n",
    "        #self.h is for the m \"h\" used at every loop, h is a configuration from the search space\n",
    "        self.h = np.repeat({},num_config) \n",
    "        \n",
    "        #self.out is for storing the result of the algorithm, ie all \"h\" from all iterations\n",
    "        #from all sqrt(m) best models per iterations.\n",
    "        self.out = np.zeros((num_iteration,self.sqrt_config))\n",
    "        \n",
    "        #self.hyperspaces is for storing the sqrt(m) hyperspaces used by the algorithm\n",
    "        self.hyperspaces = np.zeros(self.sqrt_config)\n",
    "        \n",
    "        self.plot = np.zeros(num_iteration)\n",
    "\n",
    "        \n",
    "        #self.model is the m model that will explore new hyperspace points at every iterations\n",
    "        self.models = np.repeat(model,num_config)\n",
    "        \n",
    "        #self.parents is the sqrt(m) best model from last iteration\n",
    "        self.parents = np.repeat(model,self.sqrt_config)\n",
    "\n",
    "        #self.losses remembers the performances of all m models at one iteration to decide which ones are the sqrt(m) best from self.models.\n",
    "        self.losses = np.zeros(num_config)\n",
    "        \n",
    "        self.k = [0] # c'est pour avoir un pointeur sur k, c'est pas plus que O(sqrt)-paralélisable  pour le moment du coup.\n",
    "    \n",
    "    def initialisation(self):\n",
    "        num_config = self.num_config\n",
    "        extended_Hyperspace = Trials()\n",
    "        fmin_objective = partial(test_function, models=self.models,h=self.h,losses=self.losses,parent_model=self.models, k_f = self.k,iteration = 0)\n",
    "        self.oracle.compute_batch(extended_Hyperspace ,num_config , 0 ,fmin_objective)\n",
    "            \n",
    "        indexes = np.argsort(self.losses)     \n",
    "        self.out[0] = (self.losses[indexes])[0:self.sqrt_config]\n",
    "        self.hyperspaces = np.repeat(extended_Hyperspace,self.sqrt_config)    \n",
    "        self.parents = np.array([Parent(copy.deepcopy(extended_Hyperspace),(self.h[indexes])[i], (self.models[indexes])[i],(self.losses[indexes])[i])  \n",
    "                                 for i in range(self.sqrt_config) ])         \n",
    "        self.plot[0] = self.losses[indexes][0]\n",
    "        \n",
    "    def loop(self):\n",
    "        sqrt_config = self.sqrt_config\n",
    "        iteration = self.iteration\n",
    "        for i in range(1,iteration):\n",
    "            \n",
    "            self.k[0] = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for j in range(sqrt_config):\n",
    "                parent = self.parents[j]\n",
    "                point_extended_hyperspace = parent.get_point_hyperspace()\n",
    "                print(\"\\n loss of parent \" + str(parent.get_loss()[-1]) )\n",
    "                print(\"\\n loss \" + str(parent.get_loss()))\n",
    "                \n",
    "                fmin_objective = partial(test_function, models=self.models,h=self.h,losses=self.losses,parent_model=parent.get_model(), k_f = self.k,iteration = len(parent.get_loss()))\n",
    "\n",
    "                    \n",
    "                if not parent.is_replicated:\n",
    "                   # point_extended_hyperspace = Trials()\n",
    "                   # parent.point_hyperspace = Trials()\n",
    "                    print('not replicated')\n",
    "                    self.oracle.repeat_good(\n",
    "                        point_extended_hyperspace,\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective,\n",
    "                        parent.configuration_list[-1]\n",
    "                    )\n",
    "\n",
    "                    # computes the new batch for each one of the parents for every iteration\n",
    "                    self.oracle.compute_batch(\n",
    "                        point_extended_hyperspace,\n",
    "                        int(self.num_config/self.n_parents) - 1,\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective\n",
    "                    )\n",
    "                else:\n",
    "\n",
    "                    print('replicated')\n",
    "                    self.oracle.compute_batch(\n",
    "                        point_extended_hyperspace,\n",
    "                        int(self.num_config/self.n_parents),\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective\n",
    "                    )\n",
    "                    \n",
    "\n",
    "            #self.oracle.Repeat_good(extended_Hyperspace ,i ,fmin_objective,parent.configuration_list[-1])\n",
    "             #   self.oracle.compute_Batch(extended_Hyperspace ,int(self.num_config/sqrt_config) -1 , i ,fmin_objective)\n",
    "           \n",
    "            print(\"totalt time: \" +  str(time.time() - start_time))\n",
    "\n",
    "\n",
    "            combined_losses = np.concatenate(\n",
    "                        (\n",
    "                            self.losses,\n",
    "                            \n",
    "                                [self.parents[i].get_loss()[-1].item() for i in range(self.n_parents)]\n",
    "                            \n",
    "                        ),\n",
    "                        0\n",
    "                    )\n",
    "            ixs_parents = np.argsort(combined_losses)\n",
    "            parent_idx = ixs_parents[:self.n_parents]\n",
    "            print(combined_losses)\n",
    "            print(parent_idx)\n",
    "            # ??? why saving it in a numpt array ?\n",
    "            # It is creating the new Parent `array`\n",
    "            temp_parents = [''] * self.n_parents\n",
    "\n",
    "            for j, x in enumerate(parent_idx):\n",
    "                # ??? why converting it to integer ?\n",
    "                x = int(x)\n",
    "                if x >= self.num_config:\n",
    "                    temp_parents[j] = copy.deepcopy(self.parents[x - self.num_config])\n",
    "                    temp_parents[j].replication(self.n_parents)\n",
    "                else:\n",
    "                    temp_parents[j] = copy.deepcopy(self.parents[math.floor(x/self.num_config * self.n_parents)])\n",
    "                    temp_parents[j].update(self.h[x], self.losses[x], self.models[x])\n",
    "            self.parents = temp_parents\n",
    "\n",
    "\n",
    "class FSVNLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"trash.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle (Paul) TODO\n",
    "path = \"./temp/data\"\n",
    "class Oracle():\n",
    "    def __init__(self, searchspace ):\n",
    "        #self.hyperspace is the original (input) searchspace\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def repeat_good(self,trials, iteration,function,configuration): #add space\n",
    "        space = copy.deepcopy(configuration)\n",
    "        for k,v in configuration.items():\n",
    "            space[k] =  hp.uniform(k,-1e-10+v,v + 1e-10) \n",
    "\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "    def compute_once(self,trials, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "        \n",
    "    def compute_batch(self,trials, nb_eval, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "            \n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+nb_eval, trials=trials)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1)\n",
    "    , \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1)\n",
    "          ,   \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "}\n",
    "fsvnlogger = FSVNLogger(config,\"\")\n",
    "\n",
    "    \n",
    "CONFIGURATION = 25\n",
    "ITERATIONS = 10\n",
    "\n",
    "model = train_mnist\n",
    "oracle = Oracle(config)\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "  scheduler = Scheduler(\n",
    "    model,\n",
    "    ITERATIONS,\n",
    "    CONFIGURATION,\n",
    "    oracle) \n",
    "\n",
    "  start_time = time.time()\n",
    "  scheduler.initialisation()     \n",
    "  scheduler.loop()     \n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments Mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpbthebo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "982c753f8206e818bf3c8246fb550e6cb472312c5a9734dc62e666191174e1ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
