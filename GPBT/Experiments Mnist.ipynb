{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20246,
     "status": "ok",
     "timestamp": 1617488631372,
     "user": {
      "displayName": "Julien Scardigli",
      "photoUrl": "",
      "userId": "16471186942324360260"
     },
     "user_tz": -120
    },
    "id": "e6G7RhE7s34d",
    "outputId": "12f2ef85-2672-43be-850e-109abc009761"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/gpbthebo/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import random       \n",
    "import math \n",
    "from ray import tune\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from functools import *\n",
    "from ray.tune.logger import *\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, \\\n",
    "            select_length, UCB, standardize, TV_SquaredExp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMON CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56756,
     "status": "ok",
     "timestamp": 1617488691939,
     "user": {
      "displayName": "Julien Scardigli",
      "photoUrl": "",
      "userId": "16471186942324360260"
     },
     "user_tz": -120
    },
    "id": "B6SaMRN0rPDg",
    "outputId": "ca452473-cf07-4a43-c592-5e3bb8595076"
   },
   "outputs": [],
   "source": [
    "EPOCH_SIZE = 32*32*32*32\n",
    "TEST_SIZE = 256*32*32 #remove 1024\n",
    "\n",
    "#This is a function that can be used by several NN model\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#This is a function that can be used by several NN model (it only does accuracy ATM)\n",
    "def test(model, func, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()  \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DF_euOzyrPDn"
   },
   "outputs": [],
   "source": [
    "# A random mnist from the internet to get a correct model to reason about\n",
    "\n",
    "class train_mnist():\n",
    "    def __init__(self,config):\n",
    "        self.DEFAULT_PATH = \"./tmp/data/\"\n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":43\n",
    "      ,  \"n_layer\":2    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "       # mnist_transforms = transforms.Compose(\n",
    "       #     [transforms.ToTensor(),\n",
    "       #      transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.MNIST(self.DEFAULT_PATH, train=True, download=True , transform=mnist_transforms),\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.model = LeNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def adapt(self, config):\n",
    "        #print(self.optimizer)\n",
    "        temp = copy.deepcopy(self)\n",
    "        for key, value in config.items():\n",
    "            temp.config[key] = value\n",
    "        config = temp.config\n",
    "\n",
    "        temp.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "        temp.optimizer = torch.optim.Adam(temp.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((config.get(\"b1\", 0.999),config.get(\"b2\", 0.9999))),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        self.train1()\n",
    "        return self.val1()\n",
    "\n",
    "# __INCEPTION_SCORE_begin__\n",
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "            drop_prob, sigmoid ):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def adapt(self,drop_prob):\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function, evals): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.rand.suggest), max_evals=evals, trials=Trials())\n",
    "\n",
    "class RandomLogger(tune.logger.Logger):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        timestamp = datetime.time().strftime(\"%H_%M_%d_%m_%Y\")\n",
    "        directory = os.path.join(\".temp/data\", \"MNIST\", timestamp)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = \"randomopt\" + \"_\" + \"LeNet\" + \"_\" + str(0) + \".csv\"\n",
    "        progress_file = os.path.join(directory, filename)\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal Random Opt\n",
    "def function(x, hyperlogger):\n",
    "    model = train_mnist(x)\n",
    "    for _ in range(2): # Iterations\n",
    "        model.train1()\n",
    "        loss = model.test1()\n",
    "        test = model.val1()\n",
    "        temp = dict(x)\n",
    "        temp.update({'loss' : loss})\n",
    "        temp.update({'test' : test})\n",
    "\n",
    "        temp.update({'iteration' :  model.i})\n",
    "        hyperlogger.on_result(result=temp)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0                                         \n",
      "iteration: 1                                         \n",
      "100%|██████████| 1/1 [00:19<00:00, 19.50s/trial, best loss: -0.1088]\n",
      "totalt time: 19.50960683822632\n"
     ]
    }
   ],
   "source": [
    "evals = 1  # num_configurations\n",
    "\n",
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1),\n",
    "     \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1),\n",
    "     \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "}\n",
    "\n",
    "oracle = RandomOpt(config)\n",
    "hyperlogger = RandomLogger(config)\n",
    "start_time = time.time()\n",
    "\n",
    "fmin_objective = partial(function, hyperlogger=hyperlogger)\n",
    "oracle.compute_Once(fmin_objective, evals)\n",
    "print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function, evals): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=evals, trials=Trials())\n",
    "    \n",
    "\n",
    "class BayesLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\"./tmp/data\", \"bayesopt_v2.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0                                         \n",
      "iteration: 1                                         \n",
      "100%|██████████| 1/1 [00:18<00:00, 18.86s/trial, best loss: -0.8582]\n",
      "totalt time: 18.863043069839478\n"
     ]
    }
   ],
   "source": [
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1),\n",
    "     \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1),\n",
    "     \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "    \n",
    "}\n",
    "\n",
    "hyperlogger = BayesLogger(config,\"\")\n",
    "\n",
    "for _ in range(1):\n",
    "  oracle = BayesOpt(config)\n",
    "  start_time = time.time()\n",
    "  fmin_objective = partial(function, hyperlogger=hyperlogger)\n",
    "  oracle.compute_Once(fmin_objective, evals)\n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import *\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\"./tmp/data/\", \"BOHB_LeNet.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler \n",
    "from ray.tune.schedulers import MedianStoppingRule\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "#import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "class train_mnist_pb2(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.DEFAULT_PATH = \"./tmp/data\"\n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":43\n",
    "      ,  \"n_layer\":2    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "        #mnist_transforms = transforms.Compose(\n",
    "        #    [transforms.ToTensor(),\n",
    "        #     transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.MNIST(self.DEFAULT_PATH, train=True, download=True, transform=mnist_transforms),\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.MNIST(self.DEFAULT_PATH, train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.model = LeNet(192,int(round(config.get(\"hidden_dim\",64))),10,\n",
    "                    int( round(config.get(\"n_layer\",1))),\n",
    "                    config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        if config.get(\"b1\", 0.999)>=1:\n",
    "          temp = 1 - 1e-10\n",
    "        else:\n",
    "          temp = config.get(\"b1\", 0.999)\n",
    "                \n",
    "        if config.get(\"b2\", 0.999)>=1:\n",
    "          temp1 = 1 - 1e-10\n",
    "        else:\n",
    "          temp1 = config.get(\"b2\", 0.999)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((temp,temp1)),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def reset_config(self, config):\n",
    "        if \"lr\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"lr\"] = config.get(\"lr\", 0.01)\n",
    "        if \"b1\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"betas\"] = (\n",
    "                    (1-1e-10 if config.get(\"b1\", 0.999)>=1 else config.get(\"b1\", 0.999),(1-1e-10 if config.get(\"b2\", 0.999)>=1 else config.get(\"b2\", 0.999))))\n",
    "        if \"weight_decay\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"weight_decay\"] = config.get(\"weight_decay\", 0) \n",
    "        \n",
    "        self.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "\n",
    "        self.config = config\n",
    "        return True\n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        self.train1()\n",
    "        return {'loss' : self.test1(), 'test' : self.val1()}\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optim\": self.optimizer.state_dict(),\n",
    "\n",
    "        }, path)\n",
    "\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "                  }\n",
    "\n",
    "for _ in range(1):\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "    start_time = time.time()\n",
    "    algo = TuneBOHB(metric=\"loss\", mode=\"max\")\n",
    "    bohb = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        max_t=1)\n",
    "    analysis = tune.run(train_mnist_pb2, config=config, scheduler=bohb, search_alg=algo,num_samples=1, callbacks=[CSVLoggerCallback],resources_per_trial={'cpu':8 ,'gpu': 1})\n",
    "\n",
    "    print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(\"./temp/data\", \"PB2_2_6.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "algo = ConcurrencyLimiter(HyperOptSearch(metric=\"loss\",\n",
    "    mode=\"max\"), max_concurrent=25)\n",
    "scheduler = PB2(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=2,\n",
    "    hyperparam_bounds={\n",
    "        # distribution for resampling\n",
    "     \"lr\": [0+epsilon, .1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[0+epsilon, .1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [.9, 1-epsilon]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [.99, 1-epsilon] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "    }) \n",
    "\n",
    "imageSize = 32\n",
    "cst=imageSize\n",
    "total=cst*cst*3\n",
    "def trial_name_id(trial):\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "  ray.shutdown()\n",
    "  ray.init()\n",
    "  start_time = time.time()\n",
    "  analysis = tune.run(   \n",
    "  train_mnist_pb2,\n",
    "  scheduler=scheduler,\n",
    "  reuse_actors=False,\n",
    "  search_alg=algo,\n",
    "  verbose=2,\n",
    "  checkpoint_at_end=True,\n",
    "  num_samples=28,\n",
    "  # export_formats=[ExportFormat.MODEL],\n",
    "              config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "          \n",
    "      },      stop={\n",
    "          \"training_iteration\": 10,\n",
    "      },        metric=\"loss\",\n",
    "      mode=\"max\"\n",
    ",resources_per_trial={'cpu':8 ,'gpu': 1}\n",
    "              ,     loggers=[TestLogger])\n",
    "  print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.examples.pbt_function import pbt_function\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    class TestLogger(tune.logger.Logger):\n",
    "        def _init(self):\n",
    "            progress_file = os.path.join(path, \"PBT\"+str(i)+\".csv\")\n",
    "            self._continuing = os.path.exists(progress_file)\n",
    "            self._file = open(progress_file, \"a\")\n",
    "            self._csv_out = None\n",
    "        def on_result(self, result):\n",
    "            tmp = result.copy()\n",
    "            result = flatten_dict(tmp, delimiter=\"/\")\n",
    "            if self._csv_out is None:\n",
    "                self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "                self._csv_out.writeheader()\n",
    "            self._csv_out.writerow(\n",
    "                {k: v\n",
    "                 for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "            self._file.flush()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    ray.shutdown()\n",
    "    ray.init()  # force pausing to happen for test\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "    algo = HyperOptSearch(metric=\"loss\",\n",
    "        mode=\"max\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        perturbation_interval=1,\n",
    "            time_attr=\"training_iteration\",\n",
    "\n",
    "        hyperparam_mutations={\n",
    "            # hyperparameter bounds.\n",
    "     \"lr\": [0+epsilon, .1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[0+epsilon, .1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [.9, 1-epsilon]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [.99, 1-epsilon] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "        })\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_mnist_pb2,\n",
    "  checkpoint_at_end=True,\n",
    "        scheduler=pbt,\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        search_alg = algo,\n",
    "        verbose=0,\n",
    "        stop={\n",
    "            \"training_iteration\": 10,\n",
    "        },\n",
    "        num_samples=25,\n",
    "  reuse_actors=True,\n",
    "loggers=[TestLogger],\n",
    "        \n",
    "          config= {\n",
    "  \"lr\":  tune.uniform(0+epsilon, .1) #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "  ,     \"weight_decay\":tune.uniform(0+epsilon, .1)#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    "  ,     \"b1\": tune.uniform(.9, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"b2\": tune.uniform(.99, 1-epsilon)#,1e-4), #*10 et 0\n",
    "  ,    \"droupout_prob\": tune.uniform(0+epsilon, 1-epsilon)#,1e-4), #*10 et 0\n",
    "                      },resources_per_trial={'cpu':0 ,'gpu': 1})\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle (Paul) TODO\n",
    "path = \"./temp/data\"\n",
    "class Oracle():\n",
    "    def __init__(self, searchspace ):\n",
    "        #self.hyperspace is the original (input) searchspace\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def repeat_good(self,trials, iteration,function,configuration): #add space\n",
    "        space = copy.deepcopy(configuration)\n",
    "        for k,v in configuration.items():\n",
    "            space[k] =  hp.uniform(k,-1e-10+v,v + 1e-10) \n",
    "\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "    def compute_once(self,trials, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "        \n",
    "    def compute_batch(self,trials, nb_eval, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "            \n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+nb_eval, trials=trials)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1)\n",
    "    , \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1)\n",
    "          ,   \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "}\n",
    "fsvnlogger = FSVNLogger(config,\"\")\n",
    "\n",
    "    \n",
    "CONFIGURATION = 25\n",
    "ITERATIONS = 10\n",
    "\n",
    "model = train_mnist\n",
    "oracle = Oracle(config)\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "  scheduler = Scheduler(\n",
    "    model,\n",
    "    ITERATIONS,\n",
    "    CONFIGURATION,\n",
    "    oracle) \n",
    "\n",
    "  start_time = time.time()\n",
    "  scheduler.initialisation()     \n",
    "  scheduler.loop()     \n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments Mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpbthebo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "982c753f8206e818bf3c8246fb550e6cb472312c5a9734dc62e666191174e1ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
