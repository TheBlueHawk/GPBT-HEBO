save path : ./snapshots/simplenet
{'data_path': './data/cifar.python', 'dataset': 'cifar10', 'arch': 'simplenet', 'epochs': 540, 'batch_size': 100, 'learning_rate': 0.1, 'momentum': 0.6667209874522404, 'decay': 0.002, 'print_freq': 20, 'save_path': './snapshots/simplenet', 'resume': '', 'start_epoch': 0, 'evaluate': False, 'ngpu': 1, 'workers': 2, 'manualSeed': 7043, 'use_cuda': False, 'aiteration': 0, 'drp': 0.009123972203446316, 'eps': 1.0161203860848419e-05, 'eps_arch': 0.0009529477516413932, 'lr': 1.7299102793832735e-08, 'momentum_arch': 0.002278686726146778, 'weight_decay': 0.08698983832777996}
Random Seed: 7043
python version : 3.7.7 (default, May  7 2020, 21:25:33)  [GCC 7.3.0]
torch  version : 1.6.0+cpu
cudnn  version : None
=> creating model 'simplenet'
=> network :
 simplenet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(128, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (10): BatchNorm2d(128, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)
    (13): Dropout2d(p=0.009123972203446316, inplace=False)
    (14): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(128, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(128, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (21): BatchNorm2d(256, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)
    (24): Dropout2d(p=0.009123972203446316, inplace=False)
    (25): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(256, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(256, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (30): ReLU(inplace=True)
    (31): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)
    (32): Dropout2d(p=0.009123972203446316, inplace=False)
    (33): Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (34): BatchNorm2d(512, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)
    (37): Dropout2d(p=0.009123972203446316, inplace=False)
    (38): Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1))
    (39): BatchNorm2d(2048, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (40): ReLU(inplace=True)
    (41): Conv2d(2048, 256, kernel_size=[1, 1], stride=(1, 1))
    (42): BatchNorm2d(256, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (43): ReLU(inplace=True)
    (44): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)
    (45): Dropout2d(p=0.009123972203446316, inplace=False)
    (46): Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    (47): BatchNorm2d(256, eps=0.0009529477516413932, momentum=0.002278686726146778, affine=True, track_running_stats=True)
    (48): ReLU(inplace=True)
  )
  (classifier): Linear(in_features=256, out_features=10, bias=True)
  (drp): Dropout(p=0.009123972203446316, inplace=False)
)
=> Seed '7043'
=> dataset mean and std '[0.4913725490196078, 0.4823529411764706, 0.4466666666666667] - [0.24705882352941178, 0.24352941176470588, 0.2615686274509804]'
=> optimizer '{'optimizer': {'state': {}, 'param_groups': [{'lr': 1.7299102793832735e-08, 'rho': 0.6667209874522404, 'eps': 1.0161203860848419e-05, 'weight_decay': 0.08698983832777996, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]}]}}'
=> did not use any checkpoint for simplenet model

==>>[2021-04-12 17:44:40] [Epoch=000/540] [Need: 00:00:00] [learning_rate=0.000000] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/500]   Time 2.238 (2.238)   Data 0.077 (0.077)   Loss 2.3108 (2.3108)   Prec@1 10.000 (10.000)   Prec@5 59.000 (59.000)   [2021-04-12 17:44:42]
  **Train** Prec@1 10.000 Prec@5 59.000 Error@1 90.000
  **Test** Prec@1 12.000 Prec@5 52.000 Error@1 88.000
  **Test** Prec@1 10.000 Prec@5 57.000 Error@1 90.000

==>>[2021-04-12 17:44:50] [Epoch=001/540] [Need: 00:00:00] [learning_rate=0.000000] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [001][000/500]   Time 2.811 (2.811)   Data 0.099 (0.099)   Loss 2.3284 (2.3284)   Prec@1 13.000 (13.000)   Prec@5 58.000 (58.000)   [2021-04-12 17:44:53]
  **Train** Prec@1 13.000 Prec@5 58.000 Error@1 87.000
  **Test** Prec@1 11.000 Prec@5 48.000 Error@1 89.000
  **Test** Prec@1 15.000 Prec@5 47.000 Error@1 85.000

==>>[2021-04-12 17:44:56] [Epoch=001/540] [Need: 00:00:00] [learning_rate=0.000047] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [001][000/500]   Time 2.790 (2.790)   Data 0.094 (0.094)   Loss 2.4612 (2.4612)   Prec@1 9.000 (9.000)   Prec@5 47.000 (47.000)   [2021-04-12 17:44:58]
  **Train** Prec@1 9.000 Prec@5 47.000 Error@1 91.000
  **Test** Prec@1 9.000 Prec@5 46.000 Error@1 91.000
  **Test** Prec@1 13.000 Prec@5 49.000 Error@1 87.000

==>>[2021-04-12 17:45:01] [Epoch=001/540] [Need: 00:00:00] [learning_rate=0.000014] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [001][000/500]   Time 2.876 (2.876)   Data 0.111 (0.111)   Loss 2.3394 (2.3394)   Prec@1 10.000 (10.000)   Prec@5 55.000 (55.000)   [2021-04-12 17:45:04]
  **Train** Prec@1 10.000 Prec@5 55.000 Error@1 90.000
  **Test** Prec@1 9.000 Prec@5 52.000 Error@1 91.000
  **Test** Prec@1 13.000 Prec@5 55.000 Error@1 87.000

==>>[2021-04-12 17:45:06] [Epoch=001/540] [Need: 00:00:00] [learning_rate=0.136112] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [001][000/500]   Time 2.891 (2.891)   Data 0.100 (0.100)   Loss 2.4037 (2.4037)   Prec@1 6.000 (6.000)   Prec@5 49.000 (49.000)   [2021-04-12 17:45:09]
  **Train** Prec@1 6.000 Prec@5 49.000 Error@1 94.000
  **Test** Prec@1 7.000 Prec@5 49.000 Error@1 93.000
  **Test** Prec@1 10.000 Prec@5 65.000 Error@1 90.000

==>>[2021-04-12 17:45:11] [Epoch=002/540] [Need: 00:00:00] [learning_rate=0.000000] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [002][000/500]   Time 2.786 (2.786)   Data 0.100 (0.100)   Loss 2.4711 (2.4711)   Prec@1 3.000 (3.000)   Prec@5 43.000 (43.000)   [2021-04-12 17:45:14]
  **Train** Prec@1 3.000 Prec@5 43.000 Error@1 97.000
  **Test** Prec@1 12.000 Prec@5 44.000 Error@1 88.000
  **Test** Prec@1 7.000 Prec@5 41.000 Error@1 93.000

==>>[2021-04-12 17:45:16] [Epoch=002/540] [Need: 00:00:00] [learning_rate=0.000001] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [002][000/500]   Time 2.932 (2.932)   Data 0.103 (0.103)   Loss 2.5066 (2.5066)   Prec@1 10.000 (10.000)   Prec@5 44.000 (44.000)   [2021-04-12 17:45:19]
  **Train** Prec@1 10.000 Prec@5 44.000 Error@1 90.000
  **Test** Prec@1 8.000 Prec@5 41.000 Error@1 92.000
  **Test** Prec@1 13.000 Prec@5 47.000 Error@1 87.000

==>>[2021-04-12 17:45:22] [Epoch=002/540] [Need: 00:00:00] [learning_rate=0.008390] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [002][000/500]   Time 2.867 (2.867)   Data 0.107 (0.107)   Loss 2.3930 (2.3930)   Prec@1 7.000 (7.000)   Prec@5 44.000 (44.000)   [2021-04-12 17:45:24]
  **Train** Prec@1 7.000 Prec@5 44.000 Error@1 93.000
  **Test** Prec@1 15.000 Prec@5 49.000 Error@1 85.000
  **Test** Prec@1 4.000 Prec@5 45.000 Error@1 96.000

==>>[2021-04-12 17:45:27] [Epoch=002/540] [Need: 00:00:00] [learning_rate=0.002025] [Best : Accuracy=12.00, Error=88.00]
  Epoch: [002][000/500]   Time 2.859 (2.859)   Data 0.108 (0.108)   Loss 2.3877 (2.3877)   Prec@1 9.000 (9.000)   Prec@5 55.000 (55.000)   [2021-04-12 17:45:30]
  **Train** Prec@1 9.000 Prec@5 55.000 Error@1 91.000
  **Test** Prec@1 9.000 Prec@5 53.000 Error@1 91.000
  **Test** Prec@1 12.000 Prec@5 53.000 Error@1 88.000
