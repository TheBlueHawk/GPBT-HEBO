{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,prob):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def adapt(self,prob):\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearning():      \n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.policy = Policy(config.get(\"prob\"))\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config.get(\"lr\"),\n",
    "                betas=((config.get(\"b1\", 0.999), config.get(\"b2\", 0.9999))),\n",
    "                eps=config.get(\"eps1\", 1e-08),\n",
    "                weight_decay=config.get(\"weight_decay\", 0))\n",
    "        self.eps = config.get(\"eps2\", 1e-08)\n",
    "        self.gamma = config.get(\"gamma\")\n",
    "        self.exploration = config.get(\"exploration\")\n",
    "\n",
    "        self.running_reward = 10\n",
    "        self.env.seed(543)\n",
    "\n",
    "    def select_action(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.policy.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def adapt(self,config):\n",
    "        self_copy = copy.deepcopy(self)\n",
    "        self_copy.policy.adapt(config.get(\"prob\"))\n",
    "\n",
    "        self_copy.optimizer = optim.Adam(self.policy.parameters(), lr=config.get(\"lr\"),\n",
    "                betas=((config.get(\"b1\", 0.999), config.get(\"b2\", 0.9999))),\n",
    "                eps=config.get(\"eps1\", 1e-08),\n",
    "                weight_decay=config.get(\"weight_decay\", 0))\n",
    "        self_copy.eps = config.get(\"eps2\", 1e-08)\n",
    "        self_copy.gamma = config.get(\"gamma\")\n",
    "        self_copy.exploration = config.get(\"exploration\")\n",
    "        \n",
    "        return self_copy\n",
    "    \n",
    "    \n",
    "    \n",
    "    def finish_episode(self):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.policy.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "        for log_prob, R in zip(self.policy.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del self.policy.rewards[:]\n",
    "        del self.policy.saved_log_probs[:]\n",
    "\n",
    "    def step(self):\n",
    "        for _ in range(10):\n",
    "            state, ep_reward = self.env.reset(), 0\n",
    "            for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "                action = self.select_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                self.policy.rewards.append(reward)\n",
    "                ep_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.running_reward = self.exploration * ep_reward + (1 - self.exploration) * self.running_reward\n",
    "            self.finish_episode()\n",
    "          #  if i_episode % args.get(\"log_interval\") == 0:\n",
    "          #      print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "          #            i_episode, ep_reward, self.running_reward))\n",
    "\n",
    "        return self.running_reward\n",
    "\n",
    "    \n",
    "\n",
    "torch.manual_seed(543)\n",
    "config = {\"prob\":.7,\"lr\":.01, \"b1\": 0.999, \"b2\": 0.9999,\n",
    "                \"eps\": 1e-08,\n",
    "                \"weight_decay\": 0,\"eps2\": 1e-08 ,\"eps1\": 1e-08,\"gamma\" : 0.99,\"exploration\":.05} \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.121750099806345\n",
      "18.635854642973584\n",
      "24.20474515199965\n",
      "28.147690838309018\n",
      "39.02793893504587\n",
      "40.84662878441295\n",
      "52.18124915455773\n",
      "66.85996455705856\n",
      "70.14965854777962\n",
      "66.67082286496249\n",
      "62.186129417873985\n",
      "59.3535076878001\n",
      "58.014783113753055\n",
      "63.70149607263607\n",
      "64.60618971728609\n",
      "85.11384765768422\n",
      "105.2349279993666\n",
      "130.28302168310526\n",
      "150.77848943327317\n",
      "165.30483193004332\n"
     ]
    }
   ],
   "source": [
    "RL = ReinforcementLearning(config)\n",
    "RL1 = ReinforcementLearning(config)\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(20):\n",
    "    result = RL1.step()\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
