{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6G7RhE7s34d",
    "outputId": "e8769045-3c3c-40f3-aa0c-0586b0ccc6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "import os\n",
    "\n",
    "os.chdir('/gdrive/MyDrive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGfW1rc-iP0N",
    "outputId": "10c3c948-409f-4af1-8b8a-0a202b7836e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting hebo==0.3.1\n",
      "  Downloading HEBO-0.3.1-py3-none-any.whl (100 kB)\n",
      "\u001B[K     |████████████████████████████████| 100 kB 11.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from hebo==0.3.1) (1.0.2)\n",
      "Collecting catboost>=0.24.4\n",
      "  Downloading catboost-1.1.1-cp38-none-manylinux1_x86_64.whl (76.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 76.6 MB 1.3 MB/s \n",
      "\u001B[?25hCollecting pymoo>=0.5.0\n",
      "  Downloading pymoo-0.6.0.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.6 MB 66.8 MB/s \n",
      "\u001B[?25hCollecting gpytorch>=1.4.0\n",
      "  Downloading gpytorch-1.9.0-py3-none-any.whl (245 kB)\n",
      "\u001B[K     |████████████████████████████████| 245 kB 77.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from hebo==0.3.1) (1.13.0+cu116)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from hebo==0.3.1) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from hebo==0.3.1) (1.21.6)\n",
      "Collecting GPy>=1.9.9\n",
      "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
      "\u001B[K     |████████████████████████████████| 959 kB 48.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from catboost>=0.24.4->hebo==0.3.1) (0.10.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from catboost>=0.24.4->hebo==0.3.1) (1.7.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from catboost>=0.24.4->hebo==0.3.1) (1.15.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from catboost>=0.24.4->hebo==0.3.1) (5.5.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from catboost>=0.24.4->hebo==0.3.1) (3.2.2)\n",
      "Collecting paramz>=0.9.0\n",
      "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
      "\u001B[K     |████████████████████████████████| 71 kB 11.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.8/dist-packages (from GPy>=1.9.9->hebo==0.3.1) (0.29.32)\n",
      "Collecting linear-operator>=0.1.1\n",
      "  Downloading linear_operator-0.3.0-py3-none-any.whl (155 kB)\n",
      "\u001B[K     |████████████████████████████████| 155 kB 64.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->hebo==0.3.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.1->hebo==0.3.1) (2022.6)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.8/dist-packages (from paramz>=0.9.0->GPy>=1.9.9->hebo==0.3.1) (4.4.2)\n",
      "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.8/dist-packages (from pymoo>=0.5.0->hebo==0.3.1) (1.5)\n",
      "Collecting cma==3.2.2\n",
      "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001B[K     |████████████████████████████████| 249 kB 78.3 MB/s \n",
      "\u001B[?25hCollecting alive-progress\n",
      "  Downloading alive_progress-3.0.0-py3-none-any.whl (72 kB)\n",
      "\u001B[K     |████████████████████████████████| 72 kB 1.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from pymoo>=0.5.0->hebo==0.3.1) (0.3.6)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.4->pymoo>=0.5.0->hebo==0.3.1) (0.16.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost>=0.24.4->hebo==0.3.1) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost>=0.24.4->hebo==0.3.1) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost>=0.24.4->hebo==0.3.1) (0.11.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->hebo==0.3.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->hebo==0.3.1) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.0->hebo==0.3.1) (4.4.0)\n",
      "Collecting grapheme==0.6.0\n",
      "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
      "\u001B[K     |████████████████████████████████| 207 kB 76.8 MB/s \n",
      "\u001B[?25hCollecting about-time==4.2.1\n",
      "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated->pymoo>=0.5.0->hebo==0.3.1) (1.14.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->catboost>=0.24.4->hebo==0.3.1) (8.1.0)\n",
      "Building wheels for collected packages: GPy, paramz, grapheme\n",
      "  Building wheel for GPy (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for GPy: filename=GPy-1.10.0-cp38-cp38-linux_x86_64.whl size=2783623 sha256=dbab4f0a30c3c82f80b7774ecfcaa04efc5cf04ef2fb05df93a2ee333d17b320\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/b3/22/31f07cfd7b182ea3703151b7e5a7d6447e3e1ac6aa5c529413\n",
      "  Building wheel for paramz (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=551fdc23d9b7a82513fc8f4b572cfa4d74fce2642d46dc45939749789ed96ccf\n",
      "  Stored in directory: /root/.cache/pip/wheels/66/78/6c/d98cb437834de5e29381786b4ba8a77ea68cca74653ab62713\n",
      "  Building wheel for grapheme (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210096 sha256=dea541046a55608ed8e31e40b5bf8207398b6541bba9237cc3b0a75d0b8f0e4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/08/6b/126ea9009f7482fd53a78d0db2ece5aca70af8f4a30445386b\n",
      "Successfully built GPy paramz grapheme\n",
      "Installing collected packages: grapheme, about-time, paramz, linear-operator, Deprecated, cma, alive-progress, pymoo, gpytorch, GPy, catboost, hebo\n",
      "Successfully installed Deprecated-1.2.13 GPy-1.10.0 about-time-4.2.1 alive-progress-3.0.0 catboost-1.1.1 cma-3.2.2 gpytorch-1.9.0 grapheme-0.6.0 hebo-0.3.1 linear-operator-0.3.0 paramz-0.9.5 pymoo-0.6.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hebo==0.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNfwVyHNIzhB",
    "outputId": "259f3df0-82af-4bba-a5de-6ec780f8066f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pymoo==0.5.0\n",
      "  Downloading pymoo-0.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.7 MB 29.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.8/dist-packages (from pymoo==0.5.0) (1.7.3)\n",
      "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.8/dist-packages (from pymoo==0.5.0) (3.2.2)\n",
      "Collecting cma==2.7\n",
      "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001B[K     |████████████████████████████████| 239 kB 76.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.8/dist-packages (from pymoo==0.5.0) (1.5)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from pymoo==0.5.0) (1.21.6)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.3->pymoo==0.5.0) (0.16.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->pymoo==0.5.0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->pymoo==0.5.0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->pymoo==0.5.0) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->pymoo==0.5.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3->pymoo==0.5.0) (1.15.0)\n",
      "Installing collected packages: cma, pymoo\n",
      "  Attempting uninstall: cma\n",
      "    Found existing installation: cma 3.2.2\n",
      "    Uninstalling cma-3.2.2:\n",
      "      Successfully uninstalled cma-3.2.2\n",
      "  Attempting uninstall: pymoo\n",
      "    Found existing installation: pymoo 0.6.0.1\n",
      "    Uninstalling pymoo-0.6.0.1:\n",
      "      Successfully uninstalled pymoo-0.6.0.1\n",
      "Successfully installed cma-2.7.0 pymoo-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymoo==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U_vXUCFiOcK",
    "outputId": "a10e17aa-17fb-4d48-f9c4-bb59318f92c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x\n",
      "0 -3.0\n",
      "[[11.3569]]\n",
      "After 0 iterations, best obj is 11.36\n",
      "     x\n",
      "0  0.0\n",
      "[[0.1369]]\n",
      "After 1 iterations, best obj is 0.14\n",
      "          x\n",
      "14  2.95233\n",
      "[[6.66842726]]\n",
      "After 2 iterations, best obj is 0.14\n",
      "         x\n",
      "2  0.00245\n",
      "[[0.13509326]]\n",
      "After 3 iterations, best obj is 0.14\n",
      "           x\n",
      "24 -0.418307\n",
      "[[0.62142812]]\n",
      "After 4 iterations, best obj is 0.14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from hebo.design_space.design_space import DesignSpace\n",
    "from hebo.optimizers.hebo import HEBO\n",
    "\n",
    "def obj(params : pd.DataFrame) -> np.ndarray:\n",
    "    return ((params.values - 0.37)**2).sum(axis = 1).reshape(-1, 1)\n",
    "        \n",
    "space = DesignSpace().parse([{'name' : 'x', 'type' : 'num', 'lb' : -3, 'ub' : 3}])\n",
    "opt   = HEBO(space)\n",
    "for i in range(5):\n",
    "    rec = opt.suggest(n_suggestions = 1)\n",
    "    opt.observe(rec, obj(rec))\n",
    "    print(rec)\n",
    "    print(obj(rec))\n",
    "    print('After %d iterations, best obj is %.2f' % (i, opt.y.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qe8hu1UGyi49",
    "outputId": "5614eaa9-19a1-4b66-ea21-3b28a76535a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: GPy in /usr/local/lib/python3.8/dist-packages (1.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from GPy) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from GPy) (1.7.3)\n",
      "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.8/dist-packages (from GPy) (0.29.32)\n",
      "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from GPy) (0.9.5)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.8/dist-packages (from GPy) (1.21.6)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.8/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=2dc1be8498bb2c8944cb67788cb7d748c848eba197cdbfb3dd3c772a7541cae9\n",
      "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[K     |████████████████████████████████| 125 kB 8.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install GPy\n",
    "!pip install sklearn\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B6SaMRN0rPDg",
    "outputId": "84847569-f76b-4aec-b357-6afe27f5fdfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ray==1.2.0\n",
      "  Downloading ray-1.2.0-cp38-cp38-manylinux2014_x86_64.whl (47.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 47.3 MB 1.9 MB/s \n",
      "\u001B[?25hCollecting redis>=3.5.0\n",
      "  Downloading redis-4.4.0-py3-none-any.whl (236 kB)\n",
      "\u001B[K     |████████████████████████████████| 236 kB 70.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (4.3.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (3.19.6)\n",
      "Collecting gpustat\n",
      "  Downloading gpustat-1.0.0.tar.gz (90 kB)\n",
      "\u001B[K     |████████████████████████████████| 90 kB 10.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (1.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (3.8.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (7.1.2)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (1.51.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (1.21.6)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting aioredis\n",
      "  Downloading aioredis-2.0.1-py3-none-any.whl (71 kB)\n",
      "\u001B[K     |████████████████████████████████| 71 kB 11.1 MB/s \n",
      "\u001B[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.11.0-py2.py3-none-any.whl (128 kB)\n",
      "\u001B[K     |████████████████████████████████| 128 kB 80.7 MB/s \n",
      "\u001B[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (0.15.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (6.0)\n",
      "Collecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.0 MB 63.8 MB/s \n",
      "\u001B[?25hCollecting colorful\n",
      "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
      "\u001B[K     |████████████████████████████████| 201 kB 85.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (3.8.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray==1.2.0) (2.23.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from redis>=3.5.0->ray==1.2.0) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->ray==1.2.0) (6.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray==1.2.0) (2.10)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from aioredis->ray==1.2.0) (4.4.0)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.8/dist-packages (from gpustat->ray==1.2.0) (1.15.0)\n",
      "Collecting nvidia-ml-py<=11.495.46,>=11.450.129\n",
      "  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\n",
      "Collecting psutil>=5.6.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "\u001B[K     |████████████████████████████████| 280 kB 60.5 MB/s \n",
      "\u001B[?25hCollecting blessed>=1.17.1\n",
      "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
      "\u001B[K     |████████████████████████████████| 58 kB 2.9 MB/s \n",
      "\u001B[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.8/dist-packages (from blessed>=1.17.1->gpustat->ray==1.2.0) (0.2.5)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray==1.2.0) (0.19.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray==1.2.0) (5.10.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray==1.2.0) (3.11.0)\n",
      "Collecting opencensus-context>=0.1.3\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from opencensus->ray==1.2.0) (2.8.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (2.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (1.57.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray==1.2.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray==1.2.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray==1.2.0) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray==1.2.0) (3.0.4)\n",
      "Building wheels for collected packages: gpustat\n",
      "  Building wheel for gpustat (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-1.0.0-py3-none-any.whl size=19887 sha256=55bb8c8ee992c67716e2673d1200ccec9dc82f76e406180b360e1259c20cbb97\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/ed/14/0d513c962b25da841c42022cb5847c2ef835902c8563b8fb01\n",
      "Successfully built gpustat\n",
      "Installing collected packages: psutil, opencensus-context, nvidia-ml-py, blessed, redis, py-spy, opencensus, gpustat, colorful, colorama, aioredis, aiohttp-cors, ray\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "Successfully installed aiohttp-cors-0.7.0 aioredis-2.0.1 blessed-1.19.1 colorama-0.4.6 colorful-0.5.5 gpustat-1.0.0 nvidia-ml-py-11.495.46 opencensus-0.11.0 opencensus-context-0.1.3 psutil-5.9.4 py-spy-0.3.14 ray-1.2.0 redis-4.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "psutil"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.6 MB 34.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from hyperopt) (4.64.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt) (2.8.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from hyperopt) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from hyperopt) (1.21.6)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001B[K     |████████████████████████████████| 200 kB 69.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from hyperopt) (1.7.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt) (0.16.0)\n",
      "Installing collected packages: py4j, hyperopt\n",
      "  Attempting uninstall: hyperopt\n",
      "    Found existing installation: hyperopt 0.1.2\n",
      "    Uninstalling hyperopt-0.1.2:\n",
      "      Successfully uninstalled hyperopt-0.1.2\n",
      "Successfully installed hyperopt-0.2.7 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "#The 3 following cells aim at simuling the behavior of NN models with a simple model for MNIST\n",
    "!pip install ray==1.2.0\n",
    "!pip install -U hyperopt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import math \n",
    "from ray import tune\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from functools import *\n",
    "from ray.tune.logger import *\n",
    "import time\n",
    "\n",
    "EPOCH_SIZE = 32*32*8*32\n",
    "TEST_SIZE = 256*32*32 #remove 1024\n",
    "\n",
    "#This is a function that can be used by several NN model\n",
    "def train(model, optimizer ,func ,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    #for (data, target) in train_loader:\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "           # print(\"hehe\")\n",
    "            return\n",
    "        # We set this just for the example to run quickly.\n",
    "        data = np.repeat(data, 3, 1)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "       # print(output)\n",
    "       # print(F.log_softmax(output, dim=1))\n",
    "       # print(target)\n",
    "        loss = func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#This is a function that can be used by several NN model (it only does accuracy ATM)\n",
    "def test(model, func, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            data = np.repeat(data, 3, 1)\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "\n",
    "                \n",
    "    return correct / total\n",
    "\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DF_euOzyrPDn"
   },
   "outputs": [],
   "source": [
    "# A random mnist from the internet to get a correct model to reason about\n",
    "\n",
    "class train_mnist():\n",
    "    def __init__(self,config):\n",
    "        \n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":64\n",
    "      ,  \"n_layer\":3    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "       # mnist_transforms = transforms.Compose(\n",
    "       #     [transforms.ToTensor(),\n",
    "       #      transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.FashionMNIST(path, train=True, download=True , transform=mnist_transforms),\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(\"/gdrive/MyDrive\", train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.FashionMNIST(path, train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.resnet50(num_classes = 30) #LeNet(192,64,10,\n",
    "                    #3,\n",
    "                    #config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01),  \n",
    "                                     amsgrad=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def adapt(self, config):\n",
    "        #print(self.optimizer)\n",
    "        temp = copy.deepcopy(self)\n",
    "        for key, value in config.items():\n",
    "            temp.config[key] = value\n",
    "        config = temp.config\n",
    "\n",
    "       # temp.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "        temp.optimizer = torch.optim.Adam(temp.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     amsgrad=True)\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        train1()\n",
    "        return val1()\n",
    "\n",
    "# __INCEPTION_SCORE_begin__\n",
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet for MNist classification, used for inception_score\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "            drop_prob, sigmoid ):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def adapt(self,drop_prob):\n",
    "        self.conv2_drop = nn.Dropout2d(drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Convolution Neural network using Pytorch \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, n_layers,\n",
    "                 drop_prob, sigmoid ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.sigmoid = sigmoid\n",
    "        self.i_d = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.first= nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden = [nn.Linear(hidden_dim,hidden_dim) for _ in range(self.n_layers)]\n",
    "        self.drop_out = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.last = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, self.i_d)\n",
    "        x=self.first(x)\n",
    "        x=self.drop_out(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x=self.hidden[i](x)\n",
    "            x=self.drop_out(x)\n",
    "        x = self.last(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "#class Resnet(nn.Module):\n",
    "#  def __init__(self,config):\n",
    "#    super(ConvNet, self).__init__()\n",
    "#    self.under = torchvision.models.resnet50()\n",
    "\n",
    "#  def adapt(self, config):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gGneq1DJrPDp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LMh3xPolrPDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cWCdZ07CrPDs"
   },
   "outputs": [],
   "source": [
    "path = \"/gdrive/MyDrive/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I8iL2k4_rPDt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-QSaaNdKrPDt"
   },
   "outputs": [],
   "source": [
    "class Parent():\n",
    "    \"\"\"Parent Class that handles the passage of Network Configurations from one step to the\n",
    "    following\n",
    "    \"\"\"\n",
    "    def __init__(self, point_hyperspace, configuration, model, loss):\n",
    "        self.point_hyperspace = point_hyperspace\n",
    "        self.configuration_list = [configuration]\n",
    "        self.loss_list = [np.array(loss)]\n",
    "        self.model = model\n",
    "        self.is_replicated = False\n",
    "\n",
    "    def update(self, configuration, loss, model):\n",
    "        self.is_replicated = False\n",
    "        self.configuration_list.append(configuration)\n",
    "        self.loss_list=np.append(self.loss_list,loss)\n",
    "        self.model = model\n",
    "\n",
    "    def replication(self, n_children):\n",
    "        self.is_replicated = True\n",
    "      #  self.configuration_list.append(self.configuration_list[-1])\n",
    "      #  self.loss_list=np.append(self.loss_list,self.loss_list[-1])\n",
    "    #    replication_trials(self.point_hyperspace.trials, n_children)\n",
    "\n",
    "    def get_last_conf(self):\n",
    "        return self.configuration_list[-1]\n",
    "\n",
    "    def get_point_hyperspace(self):\n",
    "        return self.point_hyperspace\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4FQcKmKrPDu",
    "outputId": "03265e65-9366-4b94-c02c-3ae3e2e33d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001B[K     |████████████████████████████████| 100 kB 9.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.21.6)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.0.2)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize\n",
    "import copy\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from hyperopt import hp, fmin,atpe ,tpe, Trials\n",
    "\n",
    "from functools import *\n",
    "#import skipy-optimize as skopt \n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_objective\n",
    "\n",
    "\n",
    "class GPGuesser():\n",
    "    \"\"\"Used to sample the hyperspace with the tools of `hyperopt`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, searchspace, verbose):\n",
    "        self.searchspace = searchspace\n",
    "        self.string = \"aiteration\"\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "    def repeat_good(self, trials, iteration, function, configuration):\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "\n",
    "     #   curr_eval = getattr(trials, '_ids')\n",
    "     #   if curr_eval == set():\n",
    "     #       curr_eval = 0\n",
    "     #   else:\n",
    "     #       curr_eval = max(curr_eval) + 1\n",
    "\n",
    "       # space[self.string] = iteration #hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        space = []\n",
    "        for k,v in configuration.items():\n",
    "            space.append(v)\n",
    "        loss=function(space)\n",
    "        space.append((iteration))\n",
    "\n",
    "        trials[0].append(space)\n",
    "        trials[1] = np.append(trials[1],loss)\n",
    "\n",
    "\n",
    "    #    print(curr_eval)\n",
    "     #   coucou = skopt.gp_minimize(function,space,n_calls=1,x0=trials[0],y0=trials[1],n_initial_points=0)\n",
    "        \n",
    "        \n",
    "      #  trials[0] = coucou.x_iters\n",
    "      #  trials[1] = coucou.func_vals\n",
    "      # # fmin(\n",
    "       #     function,\n",
    "       #     space, algo=self.algo,\n",
    "       #     max_evals=curr_eval+nb_eval,\n",
    "       #     trials=trials,\n",
    "       #     verbose=self.verbose\n",
    "       # )\n",
    "       # _ = plot_objective(coucou)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #    space = copy.deepcopy(configuration)\n",
    "\n",
    "    #    for k, v in configuration.items():\n",
    "    #        space[k] = hp.uniform(k, v - 1e-10, v + 1e-10)\n",
    "\n",
    "    #    curr_eval = getattr(trials, '_ids')\n",
    "\n",
    "    #    if curr_eval == set():\n",
    "    #        curr_eval = 0\n",
    "    #    else:\n",
    "    #        curr_eval = max(curr_eval) + 1\n",
    "\n",
    "     #   space[self.string] = iteration # hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        #print(space)\n",
    "     #   fmin(\n",
    "     #       function,\n",
    "     #       space,\n",
    "     ##       algo=self.algo,\n",
    "      #      max_evals=curr_eval+1,\n",
    "       #     trials=trials,\n",
    "       #     verbose=self.verbose\n",
    "       # )\n",
    "\n",
    "    def compute_once(self, trials, iteration, function):\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "\n",
    "        curr_eval = getattr(trials, '_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) + 1\n",
    "\n",
    "        space[self.string] = iteration # hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        fmin(\n",
    "            function,\n",
    "            space,\n",
    "            algo=self.algo,\n",
    "            max_evals=curr_eval+1,\n",
    "            trials=trials,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "    def compute_batch(self, trials, nb_eval, iteration, function):\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "\n",
    "     #   curr_eval = getattr(trials, '_ids')\n",
    "     #   if curr_eval == set():\n",
    "     #       curr_eval = 0\n",
    "     #   else:\n",
    "     #       curr_eval = max(curr_eval) + 1\n",
    "\n",
    "       # space[self.string] = iteration #hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        space.append((0,iteration+1e-10))\n",
    "      \n",
    "    #    print(curr_eval)\n",
    "        temp=0\n",
    "        if isinstance(trials[0],type(None)):\n",
    "            temp =2\n",
    "        \n",
    "        coucou = gp_minimize(function,space,n_calls=nb_eval,x0=trials[0],y0=trials[1],n_initial_points=temp)\n",
    "        \n",
    "        for i in coucou.x_iters:\n",
    "            i[-1] = iteration\n",
    "        trials[0] = coucou.x_iters\n",
    "        \n",
    "        trials[1] = coucou.func_vals\n",
    "       # fmin(\n",
    "       #     function,\n",
    "       #     space, algo=self.algo,\n",
    "       #     max_evals=curr_eval+nb_eval,\n",
    "       #     trials=trials,\n",
    "       #     verbose=self.verbose\n",
    "       # )\n",
    "        if(iteration==10):\n",
    "            \n",
    "            _ = plot_objective(coucou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mZFzmMvCon91"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "az4GRPEkqyiC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ohpr0f6NrPDu"
   },
   "outputs": [],
   "source": [
    "def translation(liste):\n",
    "   # config = {}\n",
    "  #  config[\"lr\"] = liste[0]\n",
    "    #config[\"droupout_prob\"] = liste[1]\n",
    "    #config[\"weight_decay\"] = liste[2]\n",
    "    #config[\"b1\"] = liste[3]\n",
    "    #config[\"b2\"] = liste[4]\n",
    "    return liste#config\n",
    "def test_function(x,models,h,losses, parent_model,k_f,iteration):\n",
    "    x= translation(x)\n",
    "\n",
    "    if (isinstance(k_f,list)):\n",
    "            k=k_f[0]\n",
    "            Islist = True \n",
    "    else:\n",
    "            k = k_f\n",
    "            Islist = False\n",
    "    print(k)\n",
    "    if iteration == 0:\n",
    "        models[k] = parent_model[k](x)\n",
    "    else:      \n",
    "        models[k] = parent_model.adapt(x)\n",
    "    if(Islist):\n",
    "        k_f[0] += 1\n",
    "    \n",
    "    #for key, value in x.items():\n",
    "    #        print(key + \" \"+str(x[key]))\n",
    "\n",
    "    h[k] = x\n",
    "    #start_time = time.time()\n",
    "    models[k].train1()\n",
    "    loss = models[k].test1()\n",
    "    test = models[k].val1()\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    temp = dict(x)\n",
    "    temp.update({'loss' : loss})\n",
    "    temp.update({'test' : test})\n",
    "    fsvnlogger.on_result(temp)\n",
    "\n",
    "    losses[k] = -loss\n",
    "    print(\"accuracy, \" + str(loss) + \"\\n\")\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "92BNO8jOJoHY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Oracle (Paul) TODO\n",
    "path = \"\"\n",
    "class Oracle():\n",
    "    def __init__(self, searchspace ):\n",
    "        #self.hyperspace is the original (input) searchspace\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def repeat_good(self,trials, iteration,function,configuration): #add space\n",
    "        space = copy.deepcopy(configuration)\n",
    "        for k,v in configuration.items():\n",
    "            space[k] =  hp.uniform(k,-1e-10+v,v + 1e-10) \n",
    "\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "    def compute_once(self,trials, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+1, trials=trials)\n",
    "        \n",
    "        \n",
    "    def compute_batch(self,trials, nb_eval, iteration,function): #add space\n",
    "\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        curr_eval = getattr(trials,'_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) +1\n",
    "            \n",
    "        space[\"itération\"] =  hp.quniform(\"itération\",-.5+iteration,.5+iteration, 1) \n",
    "        fmin(function, space, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=curr_eval\n",
    "+nb_eval, trials=trials)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iLOyRDCbJoHZ"
   },
   "outputs": [],
   "source": [
    "class Guesser():\n",
    "    \"\"\"Used to sample the hyperspace with the tools of `hyperopt`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, searchspace, verbose):\n",
    "        self.searchspace = searchspace\n",
    "        self.string = \"aiteration\"\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "    def repeat_good(self, trials, iteration, function, configuration):\n",
    "\n",
    "        space = copy.deepcopy(configuration)\n",
    "\n",
    "        for k, v in configuration.items():\n",
    "            space[k] = hp.uniform(k, v - 1e-10, v + 1e-10)\n",
    "\n",
    "        curr_eval = getattr(trials, '_ids')\n",
    "\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) + 1\n",
    "\n",
    "        space[self.string] = iteration # hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        #print(space)\n",
    "        fmin(\n",
    "            function,\n",
    "            space,\n",
    "            algo=self.algo,\n",
    "            max_evals=curr_eval+1,\n",
    "            trials=trials,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "    def compute_once(self, trials, iteration, function):\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "\n",
    "        curr_eval = getattr(trials, '_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) + 1\n",
    "\n",
    "        space[self.string] = iteration # hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        fmin(\n",
    "            function,\n",
    "            space,\n",
    "            algo=self.algo,\n",
    "            max_evals=curr_eval+1,\n",
    "            trials=trials,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "    def compute_batch(self, trials, nb_eval, iteration, function):\n",
    "        space = copy.deepcopy(self.searchspace)\n",
    "        print(space)\n",
    "        curr_eval = getattr(trials, '_ids')\n",
    "        if curr_eval == set():\n",
    "            curr_eval = 0\n",
    "        else:\n",
    "            curr_eval = max(curr_eval) + 1\n",
    "\n",
    "        space[self.string] = iteration #hp.quniform(self.string, -.5+iteration, .5+iteration, 1)\n",
    "        print(curr_eval)\n",
    "\n",
    "        fmin(\n",
    "            function,\n",
    "            space, algo=self.algo,\n",
    "            max_evals=curr_eval+nb_eval,\n",
    "            trials=trials,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IZmuekCs1_8C"
   },
   "outputs": [],
   "source": [
    "def set_iteration(algo,iteration):\n",
    "  algo.space.paras[\"aiteration\"].lb=iteration\n",
    "  algo.space.paras[\"aiteration\"].ub=iteration\n",
    "\n",
    "class Guesser():\n",
    "    \"\"\"Used to sample the hyperspace with the tools of `hyperopt`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, searchspace, verbose):\n",
    "        self.searchspace = searchspace\n",
    "        self.string = \"aiteration\"\n",
    "\n",
    "        self.verbose = verbose\n",
    "        print(self.searchspace)\n",
    "        self.algo = HEBO(searchspace)\n",
    "\n",
    "\n",
    "    def repeat_good(self, trials, iteration, function, configuration):\n",
    "\n",
    "        configuration = copy.deepcopy(configuration)\n",
    "        configuration[\"aiteration\"] = iteration\n",
    "        print(configuration)\n",
    "        rec = pd.DataFrame(configuration,index=[0])   \n",
    "        res = np.array([np.array([function(configuration)])])\n",
    "        self.algo.observe(rec,res)\n",
    "\n",
    "    def compute_batch(self, trials, nb_eval, iteration, function):\n",
    "        set_iteration(self.algo, iteration) \n",
    "        for i in range(nb_eval):\n",
    "         rec = self.algo.suggest(n_suggestions = 1,fix_input = {\"aiteration\":iteration})\n",
    "         rec1 = rec.to_dict()\n",
    "         for key in rec1:\n",
    "          rec1[key] = rec1[key][list(rec1[key].keys())[0]] \n",
    "         res = np.array([np.array([function(rec1)])])\n",
    "         self.algo.observe(rec,res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aCV9qeGrPDu",
    "outputId": "26b7599f-0efa-4e08-9510-d0138fbf1248",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hebo.design_space.design_space.DesignSpace object at 0x7f06ee7d6bb0>\n",
      "0\n",
      "iteration: 0\n",
      "accuracy, 0.4058\n",
      "\n",
      "1\n",
      "iteration: 0\n",
      "accuracy, 0.1064\n",
      "\n",
      "2\n",
      "iteration: 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class Scheduler():\n",
    "    def __init__(self, model, num_iteration, num_config,\n",
    "                 oracle):\n",
    "        #Oracle manages the Bayesian optimization\n",
    "        self.oracle = oracle\n",
    "        self.iteration = num_iteration\n",
    "        self.num_config = num_config \n",
    "        self.sqrt_config = 2 # math.floor(math.sqrt(num_config)) # math.ceil(num_config/5) #\n",
    "        self.n_parents = 1#self.sqrt_config\n",
    "        #self.h is for the m \"h\" used at every loop, h is a configuration from the search space\n",
    "        self.h = np.repeat({},num_config) \n",
    "        \n",
    "        #self.out is for storing the result of the algorithm, ie all \"h\" from all iterations\n",
    "        #from all sqrt(m) best models per iterations.\n",
    "        self.out = np.zeros((num_iteration,self.sqrt_config))\n",
    "        \n",
    "        #self.hyperspaces is for storing the sqrt(m) hyperspaces used by the algorithm\n",
    "        self.hyperspaces = np.zeros(self.sqrt_config)\n",
    "        \n",
    "        self.plot = np.zeros(num_iteration)\n",
    "\n",
    "        \n",
    "        #self.model is the m model that will explore new hyperspace points at every iterations\n",
    "        self.models = np.repeat(model,num_config)\n",
    "        \n",
    "        #self.parents is the sqrt(m) best model from last iteration\n",
    "        self.parents = np.repeat(model,self.sqrt_config)\n",
    "\n",
    "        #self.losses remembers the performances of all m models at one iteration to decide which ones are the sqrt(m) best from self.models.\n",
    "        self.losses = np.zeros(num_config)\n",
    "        \n",
    "        self.k = [0] # c'est pour avoir un pointeur sur k, c'est pas plus que O(sqrt)-paralélisable  pour le moment du coup.\n",
    "    \n",
    "    def initialisation(self):\n",
    "        num_config = self.num_config\n",
    "        extended_Hyperspace = Trials() #[None,None]\n",
    "        fmin_objective = partial(test_function, models=self.models,h=self.h,losses=self.losses,parent_model=self.models, k_f = self.k,iteration = 0)\n",
    "        self.oracle.compute_batch(extended_Hyperspace ,num_config , 0 ,fmin_objective)\n",
    "            \n",
    "        indexes = np.argsort(self.losses)     \n",
    "        self.out[0] = (self.losses[indexes])[0:self.sqrt_config]\n",
    "        self.hyperspaces = np.repeat(extended_Hyperspace,self.sqrt_config)    \n",
    "        self.parents = np.array([Parent(copy.deepcopy(extended_Hyperspace),(self.h[indexes])[i], (self.models[indexes])[i],(self.losses[indexes])[i])  \n",
    "                                 for i in range(self.sqrt_config) ])         \n",
    "        self.plot[0] = self.losses[indexes][0]\n",
    "        \n",
    "    def loop(self):\n",
    "        sqrt_config = self.sqrt_config\n",
    "        iteration = self.iteration\n",
    "        for i in range(1,iteration):\n",
    "            \n",
    "            self.k[0] = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for j in range(self.n_parents):\n",
    "                parent = self.parents[j]\n",
    "                point_extended_hyperspace = parent.get_point_hyperspace()\n",
    "                print(\"\\n loss of parent \" + str(parent.get_loss()[-1]) )\n",
    "                print(\"\\n loss \" + str(parent.get_loss()))\n",
    "                \n",
    "                fmin_objective = partial(test_function, models=self.models,h=self.h,losses=self.losses,parent_model=parent.get_model(), k_f = self.k,iteration = len(parent.get_loss()))\n",
    "\n",
    "                    \n",
    "                if not parent.is_replicated:\n",
    "                    print('not replicated')\n",
    "                    self.oracle.repeat_good(\n",
    "                        point_extended_hyperspace,\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective,\n",
    "                        parent.configuration_list[-1]\n",
    "                    )\n",
    "\n",
    "                    # computes the new batch for each one of the parents for every iteration\n",
    "                    self.oracle.compute_batch(\n",
    "                        point_extended_hyperspace,\n",
    "                        int(self.num_config/self.n_parents) - 1,\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective\n",
    "                    )\n",
    "                else:\n",
    "\n",
    "                    print('replicated')\n",
    "                    self.oracle.compute_batch(\n",
    "                        point_extended_hyperspace,\n",
    "                        int(self.num_config/self.n_parents),\n",
    "                        len(parent.get_loss()),\n",
    "                        fmin_objective\n",
    "                    )\n",
    "                    \n",
    "\n",
    "            #self.oracle.Repeat_good(extended_Hyperspace ,i ,fmin_objective,parent.configuration_list[-1])\n",
    "             #   self.oracle.compute_Batch(extended_Hyperspace ,int(self.num_config/sqrt_config) -1 , i ,fmin_objective)\n",
    "           \n",
    "            print(\"totalt time: \" +  str(time.time() - start_time))\n",
    "\n",
    "\n",
    "            combined_losses = np.concatenate(\n",
    "                        (\n",
    "                            self.losses,\n",
    "                            \n",
    "                                [self.parents[i].get_loss()[-1].item() for i in range(self.n_parents)]\n",
    "                            \n",
    "                        ),\n",
    "                        0\n",
    "                    )\n",
    "            ixs_parents = np.argsort(combined_losses)\n",
    "            parent_idx = ixs_parents[:self.n_parents]\n",
    "            print(combined_losses)\n",
    "            print(parent_idx)\n",
    "            # ??? why saving it in a numpt array ?\n",
    "            # It is creating the new Parent `array`\n",
    "            temp_parents = [''] * self.n_parents\n",
    "\n",
    "            for j, x in enumerate(parent_idx):\n",
    "                # ??? why converting it to integer ?\n",
    "                x = int(x)\n",
    "                if x >= self.num_config:\n",
    "                    temp_parents[j] = copy.deepcopy(self.parents[x - self.num_config])\n",
    "                    temp_parents[j].replication(self.n_parents)\n",
    "                else:\n",
    "                    temp_parents[j] = copy.deepcopy(self.parents[math.floor(x/self.num_config * self.n_parents)])\n",
    "                    temp_parents[j].update(self.h[x], self.losses[x], self.models[x])\n",
    "               #     temp_parents[j].point_hyperspace = Trials()\n",
    "\n",
    "            self.parents = temp_parents\n",
    "\n",
    "\n",
    "class FSVNLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"FSNV_MNIST_GP.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "                   \n",
    "\n",
    "config= {\n",
    "     \"lr\": hp.loguniform(\"lr\",-10*2.3,0)\n",
    "\n",
    "    #    , \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1)\n",
    "#          ,   \"weight_decay\": hp.loguniform(\"weight_decay\",-5*2.3,-1*2),\n",
    "#    \"b1\" : 1-hp.loguniform(\"b1\",-4*2.3, -1*2.3),\n",
    "#    \"b2\" : 1-hp.loguniform(\"b2\",-5*2.3, -2*2.3)\n",
    "}\n",
    "\n",
    "\n",
    "config = DesignSpace().parse([{'name' : 'lr', 'type' : 'pow', 'lb' : math.e**-10, 'ub' : 1},\n",
    "                              {'name' : 'aiteration', 'type' : 'int', 'lb' : 0, 'ub' : 0}])\n",
    "\n",
    "fsvnlogger = FSVNLogger(config,\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "CONFIGURATION = 4\n",
    "ITERATIONS = 10\n",
    "\n",
    "model = train_mnist\n",
    "oracle = Guesser(config,0)\n",
    "\n",
    "\n",
    "scheduler = Scheduler(\n",
    "  model,\n",
    "  ITERATIONS,\n",
    "  CONFIGURATION,\n",
    "  oracle) \n",
    "\n",
    "start_time = time.time()\n",
    "scheduler.initialisation()     \n",
    "scheduler.loop()     \n",
    "print(\"totalt time: \" +  str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIH42INaKZq6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHF9vlst99EB"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "\n",
    "\n",
    "  scheduler = Scheduler(\n",
    "    model,\n",
    "    ITERATIONS,\n",
    "    CONFIGURATION,\n",
    "    oracle) \n",
    "\n",
    "  start_time = time.time()\n",
    "  scheduler.initialisation()     \n",
    "  scheduler.loop()     \n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7apG6Mw9O-91"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "\n",
    "\n",
    "  scheduler = Scheduler(\n",
    "    model,\n",
    "    ITERATIONS,\n",
    "    CONFIGURATION,\n",
    "    oracle) \n",
    "\n",
    "  start_time = time.time()\n",
    "  scheduler.initialisation()     \n",
    "  scheduler.loop()     \n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8-UIcwBrPDv"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "  oracle = BayesOpt(config)\n",
    "  start_time = time.time()\n",
    "  oracle.compute_Once(function)\n",
    "  print(\"totalt time: \" +  str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWMRcnkrrPDv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGh3RPDYrPDw",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuwBv0xvrPDw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L64VWL6krPDx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEw6B1X4rPDx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmo8tGB9rPDx"
   },
   "outputs": [],
   "source": [
    "#Normal Random Opt\n",
    "evals = 4\n",
    "crypt = np.zeros(evals)\n",
    "def function(x):\n",
    "    print(x)\n",
    "    model = train_mnist(x)\n",
    "    for _ in range(20):\n",
    "        model.train1()\n",
    "        loss = model.test1()\n",
    "        test = model.val1()\n",
    "        temp = dict(x)\n",
    "        temp.update({'loss' : loss})\n",
    "        temp.update({'test' : test})\n",
    "\n",
    "        temp.update({'iteration' :  model.i})\n",
    "        hyperlogger.on_result(temp)\n",
    "    \n",
    "  #  loss = model.test1()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMRlOkQerPDx"
   },
   "outputs": [],
   "source": [
    "class RandomOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.rand.suggest), max_evals=evals, trials=Trials())\n",
    "    \n",
    "config= {\n",
    "     \"lr\": hp.uniform(\"lr\",0,.1),\n",
    "     \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1),\n",
    "     \"weight_decay\": hp.uniform(\"weight_decay\",0,.1),\n",
    "    \"b1\" : hp.uniform(\"b1\",0.9, 1),\n",
    "    \"b2\" : hp.uniform(\"b2\",0.99, 1)\n",
    "    \n",
    "}\n",
    "oracle = RandomOpt(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzDPwnT6rPDx"
   },
   "outputs": [],
   "source": [
    "class RandomLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"trash.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "                   \n",
    "hyperlogger = RandomLogger(config,\"\")\n",
    "\n",
    "start_time = time.time()\n",
    "oracle.compute_Once(function)\n",
    "print(\"totalt time: \" +  str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ELFT6MXrPDy"
   },
   "outputs": [],
   "source": [
    "class BayesOpt():\n",
    "    def __init__(self, searchspace ):\n",
    "        self.searchspace = searchspace\n",
    "\n",
    "    def compute_Once(self,function): \n",
    "        fmin(function, self.searchspace, algo=partial(tpe.suggest, n_startup_jobs=1), max_evals=evals, trials=Trials())\n",
    "    \n",
    "config= {\n",
    "     \"lr\": hp.loguniform(\"lr\",-5*2.3,-1*2)\n",
    "    , \"droupout_prob\": hp.uniform(\"droupout_prob\",0,1)\n",
    "          ,   \"weight_decay\": hp.loguniform(\"weight_decay\",-5*2.3,-1*2),\n",
    "    \"b1\" : 1-hp.loguniform(\"b1\",-4*2.3, -1*2.3),\n",
    "    \"b2\" : 1-hp.loguniform(\"b2\",-5*2.3, -2*2.3)\n",
    "}\n",
    "oracle = BayesOpt(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVUiiLoIrPDy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class HyperLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"hyperoptFMNIST.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "\n",
    "            #if not self._continuing:\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "                   \n",
    "hyperlogger = HyperLogger(config,\"\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9P5ZRXKrPDy"
   },
   "outputs": [],
   "source": [
    "# A random mnist from the internet to get a correct model to reason about\n",
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "# IN THIS MODULE: IMPORTS, CNN, TRAIN, TEST, MNIS_FUNCTION, SPACE\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler \n",
    "from ray.tune.schedulers import MedianStoppingRule\n",
    "\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#from ray.tune.suggest.skopt import SkOptSearch\n",
    "from ray import tune\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import time\n",
    "import ray\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import argparse\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "import json\n",
    "import os\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "#from ray.tune.suggest.dragonfly import DragonflySearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import torch\n",
    "#import adabelief_pytorch\n",
    "global_checkpoint_period=np.inf\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "\n",
    "class train_mnist_pb2(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        self.config = {\n",
    "        \"sigmoid_func\": 1\n",
    "      ,  \"hidden_dim\":43\n",
    "      ,  \"n_layer\":2    }\n",
    "        for key, value in config.items():\n",
    "            self.config[key] = value\n",
    "        config = self.config\n",
    "        \n",
    "        self.i = 0\n",
    "        \n",
    "        #mnist_transforms = transforms.Compose(\n",
    "        #    [transforms.ToTensor(),\n",
    "        #     transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "        mnist_transforms = transforms.ToTensor()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets.FashionMNIST(\"/gdrive/MyDrive\", train=True, download=True, transform=mnist_transforms),\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "       # self.test_loader = DataLoader(\n",
    "       #     datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "       #     batch_size=64,\n",
    "       #     shuffle=True)\n",
    "\n",
    "        \n",
    "        test_valid_dataset = datasets.FashionMNIST(\"/gdrive/MyDrive\", train=False, transform=mnist_transforms)\n",
    "        valid_ratio = 0.5  \n",
    "        nb_test = int((1.0 - valid_ratio) * len(test_valid_dataset))\n",
    "        nb_valid =  int(valid_ratio * len(test_valid_dataset))\n",
    "        test_dataset, val_dataset = torch.utils.data.dataset.random_split(test_valid_dataset, [nb_test, nb_valid])\n",
    "        self.test_loader =  DataLoader(test_dataset,\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.val_loader =  DataLoader(val_dataset,\n",
    "            batch_size=1024,\n",
    "            shuffle=True)\n",
    "\n",
    "        sigmoid_func_uniq = nn.Tanh()\n",
    "\n",
    "\n",
    "        self.model = LeNet(192,64,10,\n",
    "                    3,\n",
    "                    config.get(\"droupout_prob\",0.5) ,sigmoid_func_uniq)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        if config.get(\"b1\", 0.999)>=1:\n",
    "          temp = 1 - 1e-10\n",
    "        else:\n",
    "          temp = 1-config.get(\"b1\", 0.999)\n",
    "                \n",
    "        if config.get(\"b2\", 0.999)>=1:\n",
    "          temp1 = 1 - 1e-10\n",
    "        else:\n",
    "          temp1 = 1-config.get(\"b2\", 0.999)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.get(\"lr\", 0.01), \n",
    "                                     betas=((temp,temp1)),\n",
    "                                     eps=config.get(\"eps\", 1e-08), \n",
    "                                     weight_decay=config.get(\"weight_decay\", 0), \n",
    "                                     amsgrad=True)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def reset_config(self, config):\n",
    "        if \"lr\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"lr\"] = config.get(\"lr\", 0.01)\n",
    "        if \"b1\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"betas\"] = (\n",
    "                    (1-1e-10 if config.get(\"b1\", 0.999)>=1 else 1-config.get(\"b1\", 0.999),(1-1e-10 if config.get(\"b2\", 0.999)>=1 else 1-config.get(\"b2\", 0.999))))\n",
    "        if \"wd\" in config:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"weight_decay\"] = weight_decay\n",
    "        \n",
    "        self.model.adapt(config.get(\"droupout_prob\", 0.5))\n",
    "        self.config = config\n",
    "        return True\n",
    "    \n",
    "# All NN models should have a function train1 and test1 that calls the common train and test defined above.\n",
    "# train1 and test1 is then used in the scheduler\n",
    "    def train1(self):\n",
    "        print(\"iteration: \" + str(self.i) )\n",
    "        self.i+=1\n",
    "        train(self.model, self.optimizer, F.nll_loss, self.train_loader)\n",
    "\n",
    "    def val1(self):\n",
    "        return test(self.model, F.nll_loss, self.val_loader)\n",
    "        \n",
    "    def test1(self):\n",
    "        return test(self.model, F.nll_loss, self.test_loader)\n",
    "\n",
    "    def step(self):\n",
    "        self.train1()\n",
    "        return {'loss' : self.test1(), 'test' : self.val1()}\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optim\": self.optimizer.state_dict(),\n",
    "\n",
    "        }, path)\n",
    "\n",
    "        return checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_dir):\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optim\"])\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "\n",
    "algo = HyperOptSearch(metric=\"loss\",\n",
    "    mode=\"max\")\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=25)\n",
    "scheduler = PB2(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_bounds={\n",
    "        # distribution for resampling\n",
    "     \"lr\": [1e-5,1e-1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[1e-5,1e-1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [1e-4, 1e-1]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [1e-5, 1e-2] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "    }) \n",
    "\n",
    "imageSize = 32\n",
    "\n",
    "\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"trash.csv\") #aller jusqu'a 9\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "                   \n",
    "\n",
    "cst=imageSize\n",
    "total=cst*cst*3\n",
    "def trial_name_id(trial):\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "  ray.shutdown()\n",
    "  ray.init()\n",
    "  start_time = time.time()\n",
    "  analysis = tune.run(\n",
    "      \n",
    "  train_mnist_pb2,\n",
    "  scheduler=scheduler,\n",
    "  reuse_actors=True,\n",
    "  search_alg=algo,\n",
    "  verbose=2,\n",
    "  checkpoint_at_end=True,\n",
    "  num_samples=4,\n",
    "  # export_formats=[ExportFormat.MODEL],\n",
    "config= {\n",
    "     \"lr\": tune.loguniform(1e-5,1e-1)\n",
    "    , \"droupout_prob\": tune.uniform(0,1)\n",
    "          ,   \"weight_decay\": tune.loguniform(1e-5,1e-1),\n",
    "    \"b1\" : tune.loguniform(1e-4, 1e-1),\n",
    "    \"b2\" : tune.loguniform(1e-5, 1e-2)\n",
    "},      stop={\n",
    "          \"training_iteration\": 20,\n",
    "      },        metric=\"loss\",\n",
    "      mode=\"max\"\n",
    ",resources_per_trial={'cpu':2 ,'gpu': 1}\n",
    "              ,     loggers=[TestLogger])\n",
    "  print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exSMgkudbfLf"
   },
   "outputs": [],
   "source": [
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import *\n",
    "class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"BOHB_FMNISTvalid.csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "\n",
    "config= {\n",
    "     \"lr\": tune.loguniform(1e-5,1e-1)\n",
    "    , \"droupout_prob\": tune.uniform(0,1)\n",
    "          ,   \"weight_decay\": tune.loguniform(1e-5,1e-1),\n",
    "    \"b1\" : tune.loguniform(1e-4, 1e-1),\n",
    "    \"b2\" : tune.loguniform(1e-5, 1e-2)\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "  class TestLogger(tune.logger.Logger):\n",
    "    def _init(self):\n",
    "        progress_file = os.path.join(path, \"BOHB_FMNISTvalid\"+str(i)+\".csv\")\n",
    "        self._continuing = os.path.exists(progress_file)\n",
    "        self._file = open(progress_file, \"a\")\n",
    "        self._csv_out = None\n",
    "    def on_result(self, result):\n",
    "        tmp = result.copy()\n",
    "        result = flatten_dict(tmp, delimiter=\"/\")\n",
    "        if self._csv_out is None:\n",
    "            self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "            self._csv_out.writeheader()\n",
    "        self._csv_out.writerow(\n",
    "            {k: v\n",
    "             for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "        self._file.flush()\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "    start_time = time.time()\n",
    "    algo = TuneBOHB(metric=\"loss\", mode=\"max\")\n",
    "    bohb = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        max_t=20)\n",
    "    analysis = tune.run(train_mnist_pb2, config=config, scheduler=bohb, search_alg=algo,num_samples=4,loggers=[TestLogger],resources_per_trial={'cpu':2 ,'gpu': 1})\n",
    "\n",
    "    print(\"time \"+ str(time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtjFJVZIRGxR"
   },
   "outputs": [],
   "source": [
    "!pip install ConfigSpace\n",
    "!pip install hpbandster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58RNypJA6jT0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "from ray.tune.examples.pbt_function import pbt_function\n",
    "\n",
    "\n",
    "for i in range(9,11):\n",
    "    class TestLogger(tune.logger.Logger):\n",
    "        def _init(self):\n",
    "            progress_file = os.path.join(path, \"PBT_FMNIST\"+str(i)+\".csv\")\n",
    "            self._continuing = os.path.exists(progress_file)\n",
    "            self._file = open(progress_file, \"a\")\n",
    "            self._csv_out = None\n",
    "        def on_result(self, result):\n",
    "            tmp = result.copy()\n",
    "            result = flatten_dict(tmp, delimiter=\"/\")\n",
    "            if self._csv_out is None:\n",
    "                self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "                self._csv_out.writeheader()\n",
    "            self._csv_out.writerow(\n",
    "                {k: v\n",
    "                 for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "            self._file.flush()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    ray.shutdown()\n",
    "    ray.init()  # force pausing to happen for test\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "    algo = HyperOptSearch(metric=\"loss\",\n",
    "        mode=\"max\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        perturbation_interval=1,\n",
    "            time_attr=\"training_iteration\",\n",
    "\n",
    "        hyperparam_mutations={\n",
    "            # hyperparameter bounds.\n",
    "     \"lr\": [1e-5,1e-1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    ",     \"weight_decay\":[1e-5,1e-1]#tune.uniform(1, 5)#,1e-4), #*10 et 0\n",
    ",     \"b1\": [1e-4, 1e-1]#,1e-4), #*10 et 0\n",
    " ,    \"b2\": [1e-5, 1e-2] #,1e-4), #*10 et 0\n",
    " ,    \"droupout_prob\": [0+epsilon, 1-epsilon]#,1e-4), #*10 et 0\n",
    "        })\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_mnist_pb2,\n",
    "  checkpoint_at_end=True,\n",
    "        scheduler=pbt,\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        search_alg = algo,\n",
    "        verbose=0,\n",
    "        stop={\n",
    "            \"training_iteration\": 20,\n",
    "        },\n",
    "        num_samples=4,\n",
    "  reuse_actors=True,\n",
    "loggers=[TestLogger],\n",
    "        \n",
    "config= {\n",
    "     \"lr\": tune.loguniform(1e-5,1e-1)\n",
    "    , \"droupout_prob\": tune.uniform(0,1)\n",
    "          ,   \"weight_decay\": tune.loguniform(1e-5,1e-1),\n",
    "    \"b1\" : tune.loguniform(1e-4, 1e-1),\n",
    "    \"b2\" : tune.loguniform(1e-5, 1e-2)\n",
    "}\n",
    "                      ,resources_per_trial={'cpu':2 ,'gpu': 1})\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Un9aMJu-hxof"
   },
   "outputs": [],
   "source": [
    "from ray.tune.schedulers.pb2_utils import normalize, optimize_acq, \\\n",
    "            select_length, UCB, standardize, TV_SquaredExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeKq_qUnhoVv"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "from ray.tune.examples.pbt_function import pbt_function\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    class TestLogger(tune.logger.Logger):\n",
    "        def _init(self):\n",
    "            progress_file = os.path.join(path, \"PBT_FMNIST_CVPR\"+str(i)+\".csv\")\n",
    "            self._continuing = os.path.exists(progress_file)\n",
    "            self._file = open(progress_file, \"a\")\n",
    "            self._csv_out = None\n",
    "        def on_result(self, result):\n",
    "            tmp = result.copy()\n",
    "            result = flatten_dict(tmp, delimiter=\"/\")\n",
    "            if self._csv_out is None:\n",
    "                self._csv_out = csv.DictWriter(self._file, result.keys())\n",
    "                self._csv_out.writeheader()\n",
    "            self._csv_out.writerow(\n",
    "                {k: v\n",
    "                 for k, v in result.items() if k in self._csv_out.fieldnames})\n",
    "            self._file.flush()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    ray.shutdown()\n",
    "    ray.init()  # force pausing to happen for test\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "\n",
    "    algo = HyperOptSearch(metric=\"loss\",\n",
    "        mode=\"max\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        perturbation_interval=1,\n",
    "            time_attr=\"training_iteration\",\n",
    "\n",
    "        hyperparam_mutations={\n",
    "            # hyperparameter bounds.\n",
    "     \"lr\": [1e-10,1] #tune.uniform(1e-4, 0.1 ),#,1e-4), #*10\n",
    "\n",
    "        })\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_mnist_pb2,\n",
    "  checkpoint_at_end=True,\n",
    "        scheduler=pbt,\n",
    "        metric=\"loss\",\n",
    "        mode=\"max\",\n",
    "        search_alg = algo,\n",
    "        verbose=0,\n",
    "        stop={\n",
    "            \"training_iteration\": 10,\n",
    "        },\n",
    "        num_samples=4,\n",
    "  reuse_actors=True,\n",
    "loggers=[TestLogger],\n",
    "        \n",
    "config= {\n",
    "     \"lr\": tune.loguniform(1e-10,1)\n",
    "}\n",
    "                      ,resources_per_trial={'cpu':2 ,'gpu': 1})\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUEHaONErPDz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "print(\"Version info.\")\n",
    "print (sys.version_info)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpbthebo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "982c753f8206e818bf3c8246fb550e6cb472312c5a9734dc62e666191174e1ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
